# Adaptive Attention Span in Transformers

论文地址：

- [https://arxiv.org/abs/1905.07799](https://arxiv.org/abs/1905.07799)



## 整体思路以及计算方式

本质上是Local Attention，即计算局部注意力，改进点是给每个头一个mask，所以各个头的侧重点不同。

计算方式：

- 给定$$q, k, v\in \mathbb R^{n\times d}$$

- 计算相似度$$s_{tr}= q_t^{\top} k_r \in \mathbb R$$

- 计算mask：
  $$
  m_{z}(x)=\min \left[\max \left[\frac{1}{R}(R+z-x), 0\right], 1\right]
  $$

- 计算局部权重：
  $$
  a_{t r}=\frac{m_{z}(t-r) \exp \left(s_{t r}\right)}{\sum_{q=t-S}^{t-1} m_{z}(t-q) \exp \left(s_{t q}\right)}
  $$

- 其余部分相同



## 时间复杂度

依然是标准Attention的计算方式，所以时间复杂度为$$O(n^2 d)$$。



## 训练以及loss

loss增加了$$z$$的正则项部分：
$$
L=-\log P\left(w_{1}, \ldots, w_{T}\right)+\frac{\lambda}{M} \sum_{i} z_{i}
$$


## 代码

[https://github.com/facebookresearch/adaptive-span](https://github.com/facebookresearch/adaptive-span)



## 实验以及适用场景

Encoder和Decoder均适用；论文里测试了lm的结果，有一些提升。



## 细节

暂无。



## 简评

优点：

- 适用于单向和双向模型；
- 对每个head使用不同的mask，是一个不错的思路；

总结：

- 感觉是一个不错的思路，可以尝试复现；