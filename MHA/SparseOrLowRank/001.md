# Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection

论文地址：

- [https://arxiv.org/abs/1912.11637](https://arxiv.org/abs/1912.11637)



## 整体思路以及计算方式

对Score Matrix $$\mathbf S=\mathbf Q \mathbf K^{\top}$$增加阈值操作，小于阈值直接设置为$$-\infty$$，达到稀疏的效果。



## 时间复杂度

朴素的实现依然是$$O(n^2d)$$，利用cuda可能可以提速。



## 训练以及loss

不变。



## 代码

- [https://github.com/lancopku/Explicit-Sparse-Transformer](https://github.com/lancopku/Explicit-Sparse-Transformer)



## 实验以及适用场景

该方法适用于所有场景。



## 细节

朴素的实现应该无法提速。



## 简评

暂无。

