# Skip-Attention: Improving Vision Transformers by Paying Less Attention

论文地址：

- [https://arxiv.org/abs/2301.02240](https://arxiv.org/abs/2301.02240)



## 整体思路以及计算方式

指出Vit中很多Attention是冗余的，可以利用前几层的Attention结果加以简单的变换取代部分MHA，作者选择了中间替换中间几层的MHA，在很多任务上都能提升性能。

计算公式如下：
$$
\begin{aligned}
&\hat{Z}_l^{\mathrm{MSA}}\leftarrow \operatorname{ECA}\left(\mathrm{FC}_2\left(\operatorname{DwC}\left(\mathrm{FC}_1\left(Z_{l-1}^{\mathrm{MSA}}\right)\right)\right)\right) \\

& Z_l \leftarrow \Phi\left(Z_{l-1}^{\mathrm{MSA}}\right)+Z_{l-1} \\
& Z_l \leftarrow \operatorname{MLP}\left(Z_l\right)+Z_l
\end{aligned}
$$
图示：

![](../../.Photo/MHA/others/002.jpg)



## 简评

简单明了的一个思路。