# What Language Model to Train if You Have One Million GPU Hours?

论文地址：

- [https://arxiv.org/abs/2210.15424](https://arxiv.org/abs/2210.15424)



## 内容小结

这篇论文主要是提供一些Transformer大模型的炼丹经验，非常良心，这里总结如下：

- Attention的效率优化几乎没有啥用，用主要开销还是FFN，所以后续可以研究FFN部分的计算；
- ALibi的效果优于很多其他的位置编码；
- 把FFN换成SwiGLU，效果会有少量提升；
- 删除word embedding之后的layernorm，可以提升一定的性能；
- 多语种预训练会降低性能；