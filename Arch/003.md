# Deeper vs Wider: A Revisit of Transformer Configuration

论文地址：

- [https://arxiv.org/abs/2205.10505](https://arxiv.org/abs/2205.10505)



## 整体思路以及计算方式

一篇CV论文，核心观点是在更深更窄的网络中（参数量相当），MAE的自监督训练方式好于有监督训练。



## 时间复杂度

不考虑。



## 训练以及loss

不考虑。



## 代码

暂无。



## 实验以及适用场景

适用于所有场景，论文测试了CV和NLP掩码训练任务，（更深更窄的配置）有一定的提升，但是没有详细汇报参数数量，所以不知道结果是否公平。



## 细节

暂无。



## 简评

总的来说idea可以尝试，不过如何让网络训的更好需要研究。`****`