- [数学符号](./Notations.md)

- [Act](./Act/README.md)

  - [A survey on recently proposed activation functions for Deep Learning](./Act/001.md)

- [Arch](./Arch/README.md)

  - [Supplementary Material Implementation and Experiments for GAU-based Model](./Arch/001.md)
  - [MetaFormer is Actually What You Need for Vision](./Arch/002.md)
  - [Deeper vs Wider A Revisit of Transformer Configuration](./Arch/003.md)
  - [Perceiver General Perception with Iterative Attention](./Arch/004.md)
  - [General-purpose, long-context autoregressive modeling with Perceiver AR](./Arch/005.md)
  
- [FFN](./FFN/README.md)

  - [Large Memory Layers with Product Keys](./FFN/1.md)
- [Transformer Feed-Forward Layers Are Key-Value Memories](./FFN/2.md)
  - [GLU Variants Improve Transformer](./FFN/3.md)
- [Simple Recurrence Improves Masked Language Models](./FFN/4.md)
  - [Pay Attention to MLPs](./FFN/5.md)
- [S2-MLP Spatial-Shift MLP Architecture for Vision](./FFN/6.md)
  - [S2-MLPv2 Improved Spatial-Shift MLP Architecture for Vision](./FFN/7.md)
- [HyperMixer An MLP-based Green AI Alternative to Transformers](./FFN/8.md)
  
- 

  

  

  

  

  

