- [数学符号](./Notations.md)

- [Act](./Act/README.md)

  - [A survey on recently proposed activation functions for Deep Learning](./Act/001.md)

- [Arch](./Arch/README.md)

  - [Supplementary Material Implementation and Experiments for GAU-based Model](./Arch/001.md)
  - [MetaFormer is Actually What You Need for Vision](./Arch/002.md)
  - [Deeper vs Wider A Revisit of Transformer Configuration](./Arch/003.md)
  - [Perceiver General Perception with Iterative Attention](./Arch/004.md)
  - [General-purpose, long-context autoregressive modeling with Perceiver AR](./Arch/005.md)
  
- [FFN](./FFN/README.md)

  - [Large Memory Layers with Product Keys](./FFN/001.md)
  - [Transformer Feed-Forward Layers Are Key-Value Memories](./FFN/002.md)
  - [GLU Variants Improve Transformer](./FFN/003.md)
  - [Simple Recurrence Improves Masked Language Models](./FFN/004.md)
  - [Pay Attention to MLPs](./FFN/005.md)
  - [S2-MLP Spatial-Shift MLP Architecture for Vision](./FFN/006.md)
  - [S2-MLPv2 Improved Spatial-Shift MLP Architecture for Vision](./FFN/007.md)
  - [HyperMixer An MLP-based Green AI Alternative to Transformers](./FFN/008.md)
  
- [Head](./Head/README.md)

  - [Multi-Head Attention Collaborate Instead of Concatenate](./Head/001.md)

- [Memory](./Memory/README.md)

  - [Compressive Transformers for Long-Range Sequence Modelling](./Memory/001.md)
  - [Memformer The Memory-Augmented Transformer](./Memory/002.md)
  - [Memory Transformer](./Memory/003.md)
  - [Do Transformers Need Deep Long-Range Memory](./Memory/004.md)
  - [LaMemo Language Modeling with Look-Ahead Memory](./Memory/005.md)
  - [GMAT Global Memory Augmentation for Transformers](./Memory/006.md)
  
- [MHA](./MHA/README.md)

  - [MatrixMethod](./MHA/MatrixMethod/README.md)
    - [Skyformer Remodel Self-Attention with Gaussian Kernel and Nyström Method](./MHA/MatrixMethod/001.md)
  
- [Normalize_And_Residual](./Normalize_And_Residual/README.md)

  - [ReZero is All You Need Fast Convergence at Large Depth](./Normalize_And_Residual/001.md)
  - [Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks](./Normalize_And_Residual/002.md)
  - [Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention](./Normalize_And_Residual/003.md)
  - [RealFormer Transformer Likes Residual Attention](./Normalize_And_Residual/004.md)
  - [On Layer Normalizations and Residual Connections in Transformers](./Normalize_And_Residual/005.md)

- [Pe](./Pe/README.md)

  - [A Simple and Effective Positional Encoding for Transformers](./Pe/001.md)
  - [DeBERTa Decoding-enhanced BERT with Disentangled Attention](./Pe/002.md)
  - [DecBERT Enhancing the Language Understanding of BERT with Causal Attention Masks](./Pe/003.md)
  - [Encoding word order in complex embeddings](./Pe/004.md)
  - [Improve Transformer Models with Better Relative Position Embeddings](./Pe/005.md)
  - [KERPLE Kernelized Relative Positional Embedding for Length Extrapolation](./Pe/006.md)
  - [PermuteFormer Efficient Relative Position Encoding for Long Sequences](./Pe/007.md)
  - [Rethinking Positional Encoding in Language Pre-training](./Pe/008.md)
  - [Transformer-XL Attentive Language Models Beyond a Fixed-Length Context](./Pe/009.md)
  - [Translational Equivariance in Kernelizable Attention](./Pe/010.md)

- [Pretrain](./Pretrain/README.md)

  - [XLNet Generalized Autoregressive Pretraining for Language Understanding](./Pretrain/001.md)
  - [Transcormer Transformer for Sentence Scoring with Sliding Language Modeling](./Pretrain/002.md)
  - [Optimus Organizing Sentences via Pre-trained Modeling of a Latent Space](./Pretrain/003.md)
  - [ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators](./Pretrain/004.md)

- [Softmax](./Softmax/README.md)

  - [Transformer with a Mixture of Gaussian Keys](./Softmax/001.md)
  - [Normalized Attention Without Probability Cage](./Softmax/002.md)

- Others

  

  

  

  

  

