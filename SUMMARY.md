- [数学符号](./Notations.md)

- [Act](./Act/README.md)

  - [A survey on recently proposed activation functions for Deep Learning](./Act/001.md)

- [Arch](./Arch/README.md)

  - [Supplementary Material Implementation and Experiments for GAU-based Model](./Arch/001.md)
  - [MetaFormer is Actually What You Need for Vision](./Arch/002.md)
  - [Deeper vs Wider A Revisit of Transformer Configuration](./Arch/003.md)
  - [Perceiver General Perception with Iterative Attention](./Arch/004.md)
  - [General-purpose, long-context autoregressive modeling with Perceiver AR](./Arch/005.md)
  
- [FFN](./FFN/README.md)

  - [Large Memory Layers with Product Keys](./FFN/001.md)
  - [Transformer Feed-Forward Layers Are Key-Value Memories](./FFN/002.md)
  - [GLU Variants Improve Transformer](./FFN/003.md)
  - [Simple Recurrence Improves Masked Language Models](./FFN/004.md)
  - [Pay Attention to MLPs](./FFN/005.md)
  - [S2-MLP Spatial-Shift MLP Architecture for Vision](./FFN/006.md)
  - [S2-MLPv2 Improved Spatial-Shift MLP Architecture for Vision](./FFN/007.md)
  - [HyperMixer An MLP-based Green AI Alternative to Transformers](./FFN/008.md)
  
- [Head](./Head/README.md)

  - [Multi-Head Attention Collaborate Instead of Concatenate](./Head/001.md)

- [Memory](./Memory/README.md)

  - [Compressive Transformers for Long-Range Sequence Modelling](./Memory/001.md)
  - [Memformer The Memory-Augmented Transformer](./Memory/002.md)
  - [Memory Transformer](./Memory/003.md)
  - [Do Transformers Need Deep Long-Range Memory](./Memory/004.md)
  - [LaMemo Language Modeling with Look-Ahead Memory](./Memory/005.md)
  - [GMAT Global Memory Augmentation for Transformers](./Memory/006.md)
  
- [MHA](./MHA/README.md)

  - [MatrixMethod](./MHA/MatrixMethod/README.md)
  - [Skyformer Remodel Self-Attention with Gaussian Kernel and Nyström Method](./MHA/MatrixMethod/001.md)
  
- 

  

  

  

  

  

