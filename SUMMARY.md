- [数学符号](./Notations.md)

- [Act](./Act/README.md)

  - [A survey on recently proposed activation functions for Deep Learning](./Act/001.md)

- [Arch](./Arch/README.md)

  - [Supplementary Material Implementation and Experiments for GAU-based Model](./Arch/001.md)
  - [MetaFormer is Actually What You Need for Vision](./Arch/002.md)
  - [Deeper vs Wider A Revisit of Transformer Configuration](./Arch/003.md)
  - [Perceiver General Perception with Iterative Attention](./Arch/004.md)
  - [General-purpose, long-context autoregressive modeling with Perceiver AR](./Arch/005.md)
  - [Hierarchical Transformers Are More Efficient Language Models](./Arch/006.md)
  - [Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding](./Arch/007.md)
  - [Generalization through Memorization: Nearest Neighbor Language Models](./Arch/008.md)
  
- [FFN](./FFN/README.md)

  - [Large Memory Layers with Product Keys](./FFN/001.md)
  - [Transformer Feed-Forward Layers Are Key-Value Memories](./FFN/002.md)
  - [GLU Variants Improve Transformer](./FFN/003.md)
  - [Simple Recurrence Improves Masked Language Models](./FFN/004.md)
  - [Pay Attention to MLPs](./FFN/005.md)
  - [S2-MLP Spatial-Shift MLP Architecture for Vision](./FFN/006.md)
  - [S2-MLPv2 Improved Spatial-Shift MLP Architecture for Vision](./FFN/007.md)
  - [HyperMixer An MLP-based Green AI Alternative to Transformers](./FFN/008.md)
  - [DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling & DeLighT: Deep and Light-weight Transformer](./FFN/009.md)
  - [When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism](./FFN/010.md)
  - [Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?](./FFN/011.md)
  
- [Head](./Head/README.md)

  - [Multi-Head Attention Collaborate Instead of Concatenate](./Head/001.md)
  - [Fast Transformer Decoding: One Write-Head is All You Need](./Head/002.md)

- [Memory](./Memory/README.md)

  - [Compressive Transformers for Long-Range Sequence Modelling](./Memory/001.md)
  - [Memformer The Memory-Augmented Transformer](./Memory/002.md)
  - [Memory Transformer](./Memory/003.md)
  - [Do Transformers Need Deep Long-Range Memory](./Memory/004.md)
  - [LaMemo Language Modeling with Look-Ahead Memory](./Memory/005.md)
  - [GMAT Global Memory Augmentation for Transformers](./Memory/006.md)
  - [Block-Recurrent Transformers](./Memory/007.md)
  - [Augmenting Self-attention with Persistent Memory](./Memory/008.md)
  - [Recurrent Memory Transformer](./Memory/009.md)
  - [Memorizing Transformers](./Memory/010.md)
  
- [MHA](./MHA/README.md)

  - [FFT](./MHA/FFT/README.md)
    - [Fourier Neural Operator for Parametric Partial Differential Equations](./MHA/FFT/001.md)
    - [Global Filter Networks for Image Classification](./MHA/FFT/002.md)
    - [Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers](./MHA/FFT/003.md)
    - [FNet: Mixing Tokens with Fourier Transforms](./MHA/FFT/004.md)

  - [LocalGlobal](./MHA/LocalGlobal/README.md)
    - [CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention](./MHA/LocalGlobal/001.md)
    - [Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding](./MHA/LocalGlobal/002.md)
    - [Neighborhood Attention Transformer](./MHA/LocalGlobal/003.md)
    - [FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention](./MHA/LocalGlobal/004.md)
    - [Adaptive Attention Span in Transformers](./MHA/LocalGlobal/005.md)

  - [MatrixMethod](./MHA/MatrixMethod/README.md)
    - [Skyformer Remodel Self-Attention with Gaussian Kernel and Nyström Method](./MHA/MatrixMethod/001.md)
    - [Is Attention Better Than Matrix Decomposition](./MHA/MatrixMethod/002.md)
  
  - [RightProduct](./MHA/RightProduct/README.md)
    - [Kronecker Attention Networks](./MHA/RightProduct/001.md)
    - [An Attention Free Transformer](./MHA/RightProduct/002.md)
    - [Transformer with Fourier Integral Attentions](./MHA/RightProduct/003.md)
    - [Linear Complexity Randomized Self-attention Mechanism](./MHA/RightProduct/004.md)
    - [UFO-ViT: High Performance Linear Vision Transformer without Softmax](./MHA/RightProduct/005.md)
    - [XCiT: Cross-Covariance Image Transformers](./MHA/RightProduct/006.md)
    - [SimpleTRON: Simple Transformer with O(N) Complexity](./MHA/RightProduct/007.md)
    - [A Dot Product Attention Free Transformer](./MHA/RightProduct/008.md)
    - [On Learning the Transformer Kernel](./MHA/RightProduct/009.md)
    - [Momentum Transformer: Closing the Performance Gap Between Self-attention and Its Linearization](./MHA/RightProduct/010.md)

  - [SparseOrLowRank](./MHA/SparseOrLowRank/README.md)
    - [Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection](./MHA/SparseOrLowRank/001.md)
    - [Scatterbrain: Unifying Sparse and Low-rank Attention Approximation](./MHA/SparseOrLowRank/002.md)
    - [Sparse Factorization of Large Square Matrices](./MHA/SparseOrLowRank/003.md)
    - [Blockwise Self-Attention for Long Document Understanding](./MHA/SparseOrLowRank/004.md)
    - [H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences](./MHA/SparseOrLowRank/005.md)
    - [ChunkFormer: Learning Long Time Series with Multi-stage Chunked Transformer](./MHA/SparseOrLowRank/006.md)
    - [Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting](./MHA/SparseOrLowRank/007.md)
    - [Fast Transformers with Clustered Attention](./MHA/SparseOrLowRank/008.md)
    - [Long-Short Transformer: Efficient Transformers for Language and Vision](./MHA/SparseOrLowRank/009.md)
    - [LongT5: Efficient Text-To-Text Transformer for Long Sequences](./MHA/SparseOrLowRank/010.md)
    - [Luna: Linear Unified Nested Attention](./MHA/SparseOrLowRank/011.md)
    - [Memory-efficient Transformers via Top-k Attention](./MHA/SparseOrLowRank/012.md)
    - [Separable Self-attention for Mobile Vision Transformers](./MHA/SparseOrLowRank/013.md)
    - [Simple Local Attentions Remain Competitive for Long-Context Tasks](./MHA/SparseOrLowRank/014.md)
    - [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](./MHA/SparseOrLowRank/015.md)

  - [Others](./MHA/Others/README.md)
    - [Synthesizer: Rethinking Self-Attention in Transformer Models](./MHA/Others/001.md)
    - [Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kern](./MHA/Others/002.md)
    - [Combiner Full Attention Transformer with Sparse Computation Cost](./MHA/Others/003.md)
    - [Ripple Attention for Visual Perception with Sub-quadratic Complexity](./MHA/Others/004.md)
    - [Sinkformers: Transformers with Doubly Stochastic Attention](./MHA/Others/005.md)
    - [SOFT: Softmax-free Transformer with Linear Complexity](./MHA/Others/006.md)
    - [Value-aware Approximate Attention](./MHA/Others/007.md)
    - [EL-Attention: Memory Efficient Lossless Attention for Generation](./MHA/Others/008.md)
    - [Flowformer: Linearizing Transformers with Conservation Flows](./MHA/Others/009.md)
    - [ETSformer: Exponential Smoothing Transformers for Time-series Forecasting](./MHA/Others/010.md)
    - [IGLOO: Slicing the Features Space to Represent Sequences](./MHA/Others/011.md)
    - [Swin Transformer V2: Scaling Up Capacity and Resolution](./MHA/Others/012.md)
    - [Skip-Attention: Improving Vision Transformers by Paying Less Attention](./MHA/Others/013.md)
  
- [Normalize_And_Residual](./Normalize_And_Residual/README.md)

  - [ReZero is All You Need Fast Convergence at Large Depth](./Normalize_And_Residual/001.md)
  - [Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks](./Normalize_And_Residual/002.md)
  - [Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention](./Normalize_And_Residual/003.md)
  - [RealFormer Transformer Likes Residual Attention](./Normalize_And_Residual/004.md)
  - [On Layer Normalizations and Residual Connections in Transformers](./Normalize_And_Residual/005.md)
  - [Transformers without Tears: Improving the Normalization of Self-Attention](./Normalize_And_Residual/006.md)
  - [Query-Key Normalization for Transformers](./Normalize_And_Residual/007.md)
  - [Understanding the difficulty of training transformers](./Normalize_And_Residual/008.md)

- [Pe](./Pe/README.md)

  - [A Simple and Effective Positional Encoding for Transformers](./Pe/001.md)
  - [DeBERTa Decoding-enhanced BERT with Disentangled Attention](./Pe/002.md)
  - [DecBERT Enhancing the Language Understanding of BERT with Causal Attention Masks](./Pe/003.md)
  - [Encoding word order in complex embeddings](./Pe/004.md)
  - [Improve Transformer Models with Better Relative Position Embeddings](./Pe/005.md)
  - [KERPLE Kernelized Relative Positional Embedding for Length Extrapolation](./Pe/006.md)
  - [PermuteFormer Efficient Relative Position Encoding for Long Sequences](./Pe/007.md)
  - [Rethinking Positional Encoding in Language Pre-training](./Pe/008.md)
  - [Transformer-XL Attentive Language Models Beyond a Fixed-Length Context](./Pe/009.md)
  - [Translational Equivariance in Kernelizable Attention](./Pe/010.md)
  - [Transformer Language Models without Positional Encodings Still Learn Positional Information](./Pe/011.md)
  - [Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding](./Pe/012.md)

- [Pretrain](./Pretrain/README.md)

  - [XLNet Generalized Autoregressive Pretraining for Language Understanding](./Pretrain/001.md)
  - [Transcormer Transformer for Sentence Scoring with Sliding Language Modeling](./Pretrain/002.md)
  - [Optimus Organizing Sentences via Pre-trained Modeling of a Latent Space](./Pretrain/003.md)
  - [ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators](./Pretrain/004.md)

- [Softmax](./Softmax/README.md)

  - [Transformer with a Mixture of Gaussian Keys](./Softmax/001.md)
  - [Normalized Attention Without Probability Cage](./Softmax/002.md)

- [Others](./Others/README.md)
  - [Accelerating Neural Transformer via an Average Attention Network](./Others/001.md)
  - [Do Transformer Modifications Transfer Across Implementations and Applications?](./Others/002.md)
  - [Object-Centric Learning with Slot Attention](./Others/003.md)
  - [Do Transformer Modifications Transfer Across Implementations and Applications?](./Others/004.md)
  - [Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries](./Others/005.md)

- [LongConv](./LongConv/README.md)
  
  - [Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks](./LongConv/001.md)
  - [Parallelizing Legendre Memory Unit Training](./LongConv/002.md)
  - [Simplified State Space Layers for Sequence Modeling](./LongConv/003.md)
  - [Pretraining Without Attention](./LongConv/004.md)
  - [What Makes Convolutional Models Great on Long Sequence Modeling?](./LongConv/005.md)
  - [Hungry Hungry Hippos: Towards Language Modeling with State Space Models](./LongConv/006.md)
  - [Hyena Hierarchy: Towards Larger Convolutional Language Models](./LongConv/007.md)
  - [RWKV](./LongConv/008.md)
  - [Simple Hardware-Efficient Long Convolutions for Sequence Modeling](./LongConv/009.md)
  
- [Rnn](./Rnn/README.md)

  - [When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute](./Rnn/001.md)
  - [Linear Transformers Are Secretly Fast Weight Programmers](./Rnn/002.md)
  - [Going Beyond Linear Transformers with Recurrent Fast Weight Programmers](./Rnn/003.md)
  
- [FoundationModel](./FoundationModel/README.md)

  - [What Language Model to Train if You Have One Million GPU Hours?](./FoundationModel/001.md)
  - [Cramming: Training a Language Model on a Single GPU in One Day](./FoundationModel/002.md)

- 

- 

  

  

  

  

  

