# Neural Machine Translation in Linear Time

论文地址：

- [https://arxiv.org/abs/1610.10099](https://arxiv.org/abs/1610.10099)



## 整体思路以及计算方式

对这篇主要感兴趣的点是利用非CrossAttention的方式做多输入的交互，即Encoder-Decoder Attention的部分是否有替代方案。

先简叙下问题描述，给定输入$$\mathbf X\in \mathbb R^{n\times d}$$，输出$$\mathbf Y\in \mathbb R^{m\times d}$$，我们希望构造$$\mathbf X \to \mathbf Y$$的映射，这里作者采用的方案如下，首先将输入输出padding到相同长度：
$$
\mathbf X \to \mathbf X_1 \in \mathbb R^{l\times d},\mathbf Y\to \mathbf Y_1 \in \mathbb R^{l\times d}, l \ge n, m
$$
然后作者希望建立$$\mathbf X_1 \to \mathbf Y_1 $$的映射关系，这里用代码说明可能更直观：

```python
x_pad, y_pad = data
x1 = encoder(x_pad)
y1 = decoder(x1)
loss = criterion(y1, y_pad)
```



## 代码

- [https://github.com/dhpollack/bytenet.pytorch](https://github.com/dhpollack/bytenet.pytorch)



## 简评

总感觉这个思路不太对，有以下几点：

- 强行对齐有点过于粗暴，感觉没有抓住核心问题；
- decoder端的每次输出之和encoder部分有关，但是和之前时刻无关，感觉不太合理；