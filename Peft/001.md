# Parameter-Efficient Fine-Tuning without Introducing New Latency

论文地址：

- [https://arxiv.org/pdf/2305.16742.pdf](https://arxiv.org/pdf/2305.16742.pdf)



## 整体思路以及计算方式

一种PEFT的思路，整体思路分为两步：

- 选择进行ft的weight，方法是根据预训练的weight模长大小，选择最小的几个weight进行ft；

- ft依然是利用adapter的思路：

  - $$
    \boldsymbol{W} \leftarrow \boldsymbol{W}+f\left(\boldsymbol{W} \boldsymbol{W}_{d o w n}\right) \boldsymbol{W}_{u p}
    $$

第二步和LoRA很像，感觉复杂化了。



## 代码

- [https://github.com/baohaoliao/pafi_hiwi](https://github.com/baohaoliao/pafi_hiwi)



## 简评

还可以的思路，sparse + adapter。