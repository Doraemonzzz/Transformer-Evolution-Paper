# Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning

论文地址：

- [https://arxiv.org/pdf/2306.00477.pdf](https://arxiv.org/pdf/2306.00477.pdf)



## 整体思路以及计算方式

在介绍论文之前，首先回顾一下可逆神经网络，其思路很简单：

我们知道在前向传播中，第$$i$$层的结果可以由第$$i-1$$层（前一层）计算得到；但是在反向传播时，计算第$$i$$层的结果$$\nabla^{i} \mathcal L$$需要第$$i+1$$层（前一层）的梯度$$\nabla^{i+1} \mathcal L$$，和前向计算的中间结果$$f^{i}$$。在一般情况下，$$f^{i}$$无法通过$$f^{i+1}$$计算得到，所以$$f^{i}$$是必须要缓存的，可能神经网络就是通过巧妙的设计，使得$$f^{i}$$可以通过$$f^{i+1}$$计算得到，从而减少了内存开销。其思路如下，首先输入部分变成两个部分：
$$
\begin{aligned}
\boldsymbol{h}_{n+1}^1 & =\lambda \boldsymbol{h}_n^1+\mathcal{F}_n\left(\boldsymbol{h}_n^2\right) \\
\boldsymbol{h}_{n+1}^2 & =\beta \boldsymbol{h}_n^2+\mathcal{G}_n\left(\boldsymbol{h}_{n+1}^1\right)
\end{aligned}
$$
其中$$\mathcal F_n, \mathcal G_n$$是两个任意函数，根据上式可得：
$$
\begin{aligned}
\boldsymbol{h}_n^2 & =\left(\boldsymbol{h}_{n+1}^2-\mathcal{G}_n\left(\boldsymbol{h}_{n+1}^1\right)\right) / \beta \\
\boldsymbol{h}_n^1 & =\left(\boldsymbol{h}_{n+1}^1-\mathcal{F}_n\left(\boldsymbol{h}_n^2\right)\right) / \lambda
\end{aligned}
$$
本文主要就是基于可逆神经网络进行adapter设计，作者指出当前的PEFT并不一定能省内存，然后利用可逆神经网络缓解了这点。



## 代码

- [https://github.com/BaohaoLiao/mefts](https://github.com/BaohaoLiao/mefts)



## 简评

作者提出的方法在inference的时候会降低速度，也许是一个可改进的点。