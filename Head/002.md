# Fast Transformer Decoding: One Write-Head is All You Need

论文地址：

- [https://arxiv.org/abs/1911.02150](https://arxiv.org/abs/1911.02150)



## 整体思路以及计算方式

只将Query变成多头，Key和Value都是单头，最后的结果是性能接近，显存降低不少。



## 时间复杂度

不变。



## 训练以及loss

不变。



## 代码

暂无，但是原论文有伪代码。



## 实验以及适用场景

测试了机器翻译和lm，性能如之前所述。



## 细节

论文也测试了local attention，最后的性能相当。



## 简评

提速其实不太明显，但是总体来说，值得复现。