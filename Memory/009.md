# Recurrent Memory Transformer

论文地址：

- [https://arxiv.org/abs/2207.06881](https://arxiv.org/abs/2207.06881)



## 整体思路以及计算方式

对Transformer-XL做了改进，增加了读写内存（可微），整体流程如下：

- 输入：$$\mathbf X\in \mathbb R^{n\times d}$$；
- 记忆Token：$$\mathbf M\in \mathbb R^{m\times d}$$；
- 拼接：$$\mathbf Y= [\mathbf M , \mathbf X, \mathbf M]\in \mathbb R^{(n+2m)\times d}$$；
- $$\mathbf O =\mathrm{Tran}(\mathbf Y, \mathbf Y)\in \mathbb R^{(n+2m)\times d}$$；
- 下一层的记忆Token为：$$\mathbf M= \mathbf O[-m:,:]\in \mathbb R^{m\times d}$$

图示：

![](../.Photo/Memory/3.jpg)



## 时间复杂度

$$O((n+m)^2 d)$$。



## 训练以及loss

不变。



## 代码

- [https://github.com/booydar/transformer-xl](https://github.com/booydar/transformer-xl)



## 实验以及适用场景

适用于Encoder和Decoder。



## 细节

暂无。



## 简评

整体思路还是增加一些特殊的token。