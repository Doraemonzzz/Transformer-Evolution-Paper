# Scaling Transformer to 1M tokens and beyond with RMT

论文地址：

- [https://arxiv.org/abs/2304.11062](https://arxiv.org/abs/2304.11062)

参考资料：

- [https://zhuanlan.zhihu.com/p/624487780](https://zhuanlan.zhihu.com/p/624487780)
- [https://zhuanlan.zhihu.com/p/624451562](https://zhuanlan.zhihu.com/p/624451562)



## 整体思路以及计算方式

基于Recurrent Memory Transformer的思路进行一些toy example的验证，可以参考之前写的笔记。



## 简评

任务设定比较简单，但是可以测一下LM外推性的效果。
