# Memorizing Transformers

论文地址：

- [https://arxiv.org/abs/2203.08913](https://arxiv.org/abs/2203.08913)



## 整体思路以及计算方式

通过增加外部内存，在做Attention之前，利用Query从外部内存中找到Top-k的Key, Value，拼接到原始的Key, Value上，后续和传统Attention一致。



## 代码

- [https://github.com/lucidrains/memorizing-transformers-pytorch](https://github.com/lucidrains/memorizing-transformers-pytorch)



## 简评

和Knn-LM思路类似，不过融合信息的方式略有不同。