# A Simple and Effective Positional Encoding for Transformers

论文地址：

- [https://arxiv.org/abs/2104.08698](https://arxiv.org/abs/2104.08698)



## 整体思路以及计算方式

作者提出加性位置编码会增加矩阵的秩，具体来说，定义
$$
\begin{aligned}
    \mathbf{A}_{a}&=(\mathbf{X}+\mathbf{P}) \mathbf{W}_{Q} \mathbf{W}_{K}^{\top}(\mathbf{X}+\mathbf{P})^{\top}\\
    \mathbf{A}_{r}&=\mathbf{X} \mathbf{W}_{Q} \mathbf{W}_{K}^{\top} \mathbf{X}^{\top}+\hat{\mathbf{P}} \hat{\mathbf{P}}^{\top}
    \end{aligned}
$$
那么
$$
\inf (\mathrm{rank}(\mathbf A_r)) > \mathrm{rank}(\mathbf A_a)
$$
作者默认秩越大，性能越好，于是定义了两种位置编码方式：
$$
\begin{aligned}
\mathbf{A}_{i, j}^{\mathbf{ABS}} &=\left(\mathbf{X}_{i:} \mathbf{W}_{Q}\right)\left(\mathbf{X}_{j:} \mathbf{W}_{K}\right)^{\top} / \sqrt{d} +\left(\mathbf{P}_{Q} \mathbf{P}_{K}^{\top}\right)_{i, j}+E_{S}(S(i), S(j)) \\
\mathbf{A}_{i, j}^{\mathbf{REL}} &=\left(\mathbf{X}_{i:} \mathbf{W}_{Q}\right)\left(\mathbf{X}_{j:} \mathbf{W}_{K}\right)^{\top} / \sqrt{d} +\mathbf{R}_{i-j}+E_{S}(S(i), S(j)) \\
E_{S}(S(i), S(j))&=\mathbf{S}_{\hat{i}, \hat{j}}
\end{aligned}
$$
其中$$S$$为映射函数，$$\mathbf S$$为可学习的矩阵。



## 时间复杂度

理论复杂度不变，实际增加的计算开销微乎其微。



## 训练以及loss

不变。



## 代码

暂无，但是很好实现。



## 实验以及适用场景

适用于所有场景，可以带来一定提升，但是并不明显。



## 细节

暂无。



## 简评

个人感觉，本文的先验假设：过Softmax之前的矩阵秩越大，模型性能就越好这点站不住脚，因为即使是秩很小的矩阵，过完Softmax之后一般秩也会增加。