# Transformer Language Models without Positional Encodings Still Learn Positional Information

论文地址：

- [https://arxiv.org/abs/2203.16634](https://arxiv.org/abs/2203.16634)



## 整体思路以及计算方式

提供了一个让人有点惊讶的结论，在单向语言模型中，不需要显式地提供位置信息，模型就可以学习到位置信息，原因应该是和计算方式有关。



## 时间复杂度

不考虑。



## 训练以及loss

不考虑。



## 代码

暂无，实现起来很简单。



## 实验以及适用场景

论文测试了Decoder，Encoder情形应该不适用。



## 细节

暂无。



## 简评

一个多少有点反直觉的结论，但是信息量很多，可以考虑复现。