# Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks

论文地址：

- [https://papers.nips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html](https://papers.nips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html)



## 整体思路以及计算方式

一种RNN结构，模型计算的是输入信号在某个窗口内关于勒让德多项式的系数，达到某种程度的最优，系数的微分方程为：
$$
\theta \dot{\mathbf{m}}(t)=\mathbf{A} \mathbf{m}(t)+\mathbf{B} u(t)
$$
其中：
$$
\begin{aligned}
\mathbf{A}&=[a]_{i j} \in \mathbb{R}^{d \times d}, \quad a_{i j}=(2 i+1) \begin{cases}-1 & i<j \\
(-1)^{i-j+1} & i \geq j\end{cases} \\
\mathbf{B}&=[b]_{i} \in \mathbb{R}^{d \times 1}, \quad b_{i}=(2 i+1)(-1)^{i}, \quad i, j \in[0, d-1]
\end{aligned}
$$
离散化可得：
$$
\mathbf{m}_{t}=\mathbf{\overline A} \mathbf{m}_{t-1}+\mathbf{\overline B} u_{t}
$$
其中：
$$
{\mathbf{\overline A}}=(\Delta t / \theta) \mathbf{A}+\mathbf{I}, \quad 
{\mathbf{\overline B}}=(\Delta t / \theta) \mathbf{B}
$$
最后的模型结构为：
$$
\mathbf{h}_{t}=f\left(\mathbf{W}_{\mathbf{x}} \mathbf{x}_{t}+\mathbf{W}_{\mathbf{h}} \mathbf{h}_{t-1}+\mathbf{W}_{\mathbf{m}} \mathbf{m}_{t}\right)\\
u_{t}=\mathbf{e}_{\mathbf{x}}^{\top} \mathbf{x}_{t}+\mathbf{e}_{\mathbf{h}}{ }^{\top} \mathbf{h}_{t-1}+\mathbf{e}_{\mathbf{m}}^{\top} \mathbf{m}_{t-1}
$$



## 代码

- [https://github.com/hrshtv/pytorch-lmu](https://github.com/hrshtv/pytorch-lmu)
- [https://github.com/nengo/keras-lmu](https://github.com/nengo/keras-lmu)



## 简评

感觉S4应该或多或少从这篇文章受到某些启发。