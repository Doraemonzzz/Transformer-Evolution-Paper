# Cramming: Training a Language Model on a Single GPU in One Day

论文地址：

- [https://arxiv.org/abs/2212.14034](https://arxiv.org/abs/2212.14034)



## 内容小结

提供了非常详细的Bert炼丹经验，这里总结一下：

- BPE的词表大小使用默认的32768即可，更小的词表会降低性能，更大的效果也没有更好；
- 在Token中添加`<sep>`的性能影响很小，`<cls>`的性能影响几乎没有；
- 模型结构没那么重要，模型结构影响的是收敛速度，重要的是参数量，相同参数量的模型最后的性能相当；
  - 更深更窄的模型没有提升；
  - 只使用少量的FFN也几乎没有提升；
- Attention：各种Efficient Attention机制用处不大，因为作者的实验场景为$n=128$，RoPE有一定性能，但是会增加时间复杂度；
- FFN：使用默认的FFN即可，GLU会有少量提升；
  - 激活函数使用GELU即可；
- Embedding：使用scaled sinusodial PE；
  - 是指使用默认的Sinusoidal位置编码，然后增加一个可学的参数；
- Layer Norm：Prenorm，其他各种花哨的技术都效果不大；
  - Prenorm可以让训练更稳定，增大学习率，减少warmup轮数；
- Head：
  - 没有完全搞明白；
- Mask Rate使用Bert的默认配置即可；
- 优化器使用默认的Adam即可；
- LR Schedule：使用one-cycle learning rate；
- Batch Size随着训练轮数增加而增加，达到预设值后不变；
- Dropout：不使用；
- 数据预处理：挺重要的，可以参考论文；